#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import print_function

import sys
import re
import click
import json
import csv

from yaml import load
try:
    from yaml import CLoader as Loader
except ImportError:
    from yaml import Loader

import ruamel.yaml

from terminaltables import SingleTable

import shortuuid
import jaconv

DEFAULT_DBFILE = 'animedb.yml'
DEFAULT_CONVERTED_FILE = 'animedb.converted.yml'
DEFAULT_MADB_FILE = 'madb.json'
DEFAULT_MADB_OUTPUT_FILE = 'madb-converted.yml'
DEFAULT_SORT_KEYS = ['started_year', 'started_month', 'started_day']
DEFAULT_MERGE_KEY = 'madb_id'
DEFAULT_LIST_FORMAT = 'default'
DEFAULT_MADB_COMPARE_OUTPUT_FILE = './madb/comparison.csv'
DEFAULT_ID_NUM = 5
ID_LENGTH = 11
BASE57_CHARACTERS = '23456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz'

MEDIA = [
  u'TV',
  u'OVA',
  u'劇場',
  u'TVスペシャル',
  u'その他',
  u'イベント',
  u'個人制作',
  u'不明',
  u'該当なし'
]

CSV_FIELDNAMES = [
  u'madb_id',
  u'medium',
  u'duplicated',
  u'id',
  u'title',
  u'suffix',
  u'ruby',
  u'started_year',
  u'started_month',
  u'started_day',
  u'ended_year',
  u'ended_month',
  u'ended_day',
  u'broadcast',
  u'author',
  u'original_title',
  u'madb_uri',
  u'official_uri',
  u'memo'
]

def eprint(*args, **kwargs):
  print(*args, file=sys.stderr, **kwargs)

def load_db(dbfile):
  return load(dbfile, Loader=Loader)

def load_db_roundtrip(dbfile):
  return ruamel.yaml.load(dbfile, ruamel.yaml.RoundTripLoader)

# Load JSON MADB data output by:
#   https://github.com/builtinnya/mediaarts-db-crawler
def load_madb(dbfile):
  return json.load(dbfile)

def dump_db_roundtrip(data, outfile):
  ruamel.yaml.dump(data, outfile, Dumper=ruamel.yaml.RoundTripDumper, allow_unicode=True)

def format_date(year, month, day):
  return u'/'.join(map(unicode, filter(bool, [year, month, day])))

def format_percentage(percentage):
  return '{0:.2%}'.format(percentage)

def sort_data(data, keys=DEFAULT_SORT_KEYS):
  return sorted(data, key = lambda datum: map(lambda k: datum[k], keys))

def merge_data(data, source, key=DEFAULT_MERGE_KEY):
  for datum in data:
    if datum[key] in source:
      datum.update(source[datum[key]])

  return data

# Takes a list and returns a list which contains only truthy values in the given list.
def compact(l):
  return [v for v in l if v]

def unicode_csv_dict_reader(utf8_file, **kwargs):
  reader = csv.DictReader(utf8_file, **kwargs)

  for row in reader:
    yield { unicode(key, 'utf-8'): unicode(value, 'utf-8') for key, value in row.iteritems() }

def generate_id():
  s = shortuuid.uuid()

  return s[:ID_LENGTH]

def is_id(s):
  return bool(re.match(ur'^[{0}]{{{1}}}$'.format(BASE57_CHARACTERS, ID_LENGTH), s))

def unicode_csv_dict_writer(utf8_file, fieldnames, data, **kwargs):
  # Excel needs BOM to open UTF-8 file properly.
  # Use LibreOffice instead if you are using Excel for Mac.
  utf8_file.write(u'\ufeff'.encode('utf-8'))

  writer = csv.DictWriter(utf8_file, fieldnames, **kwargs)

  writer.writeheader()

  for datum in data:
    for key, val in list(datum.items()):
      if isinstance(key, unicode):
        key = key.encode('utf-8')

      if isinstance(val, unicode):
        val = val.encode('utf-8')

      datum[key] = val

    writer.writerow({ key: datum.get(key) for key in fieldnames })

@click.group()
def cli():
  pass

@cli.command('generate_ids')
@click.option('--num', default=DEFAULT_ID_NUM, help='The number of IDs to be generated.')
def generate_ids(num):
  for _ in range(num):
    print(generate_id())

@cli.command('attach_ids')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to attach IDs to.')
@click.option('--outputfile', default=DEFAULT_CONVERTED_FILE, type=click.File('w'), help='Output file name.')
def attach_ids(dbfile, outputfile):
  data = load_db_roundtrip(dbfile)

  for datum in data:
    current_id = datum.get(u'id')

    if not current_id or not is_id(current_id):
      datum[u'id'] = generate_id()

  dump_db_roundtrip(data, outputfile)

@cli.command('madb')
@click.option('--dbfile', default=DEFAULT_MADB_FILE, type=click.File('r'), help='MADB JSON data from crawler.')
@click.option('--outputfile', default=DEFAULT_MADB_OUTPUT_FILE, type=click.File('w'), help='Output file name.')
def madb(dbfile, outputfile):
  # Takes an episode and pre-parsed times and returns the start and end time of
  # the given episode.
  def parse_time(episode, times):
    episode_seq = re.search(ur'\d+', episode.get(u'各話表記', u''))

    if not times or not episode_seq:
      return None, None

    episode_seq = int(episode_seq.group(0))

    for first, last, start_h, start_m, end_h, end_m in times:
      if int(first) <= episode_seq and episode_seq <= int(last):
        return u':'.join([start_h, start_m]), u':'.join([end_h, end_m])

    return None, None

  data = load_madb(dbfile)
  times_re = re.compile(ur'（(\d+)(?:話|回)～(\d+)(?:話|回)）.*?(\d{1,2})時(\d{1,2})分～(\d{1,2})時(\d{1,2})分')
  result = {}

  for datum in data:
    if datum.get(u'アニメシリーズID') and datum.get(u'各話情報'):
      times_raw = datum.get(u'放送期間など', u'')
      times = times_re.findall(times_raw)
      episodes = []

      for episode in datum[u'各話情報']:
        prefix = episode.get(u'各話表記')
        title = episode.get(u'各話タイトル')
        literal_title = ' '.join(compact([prefix, title]))

        start_time, end_time = parse_time(episode, times)
        started_at = ' '.join(compact([episode.get(u'公開日'), start_time]))
        ended_at = ' '.join(compact([episode.get(u'公開日'), end_time]))

        episodes.append({
          'prefix': prefix,
          'title': title,
          'literal_title': literal_title,
          'started_at': started_at,
          'ended_at': ended_at,
          'ruby': ''
        })

      result[datum[u'アニメシリーズID']] = { 'episodes': episodes }

  dump_db_roundtrip(result, outputfile)

@cli.command('merge')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to merge in.')
@click.option('--key', default=DEFAULT_MERGE_KEY, type=str, help='Merge key.')
@click.argument('sourcefile', type=click.File('r'))
@click.argument('outputfile', type=click.File('w'))
def merge(dbfile, key, sourcefile, outputfile):
  data = load_db_roundtrip(dbfile)
  source = load_db_roundtrip(sourcefile)

  dump_db_roundtrip(merge_data(data, source, key), outputfile)

@cli.command('compare_madbs')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to compare')
@click.option('--outputfile', default=DEFAULT_MADB_COMPARE_OUTPUT_FILE, type=click.File('w'), help='Output file name.')
@click.argument('madb_files', nargs=-1, type=click.File('r'))
def compare_madbs(dbfile, outputfile, madb_files):
  anime_db = load_db(dbfile)
  madbs = []

  for madb_file in madb_files:
    madb = {}

    for row in unicode_csv_dict_reader(madb_file):
      madb_id = row[u'アニメシリーズID']

      # Skip records that don't have madb_id
      if not madb_id:
        continue

      madb[madb_id] = row

    madbs.append(madb)

  comp_fieldnames = []

  for i in range(len(madbs)):
    comp_fieldnames += [
      u'madb_{0}_title'.format(i + 1),
      u'madb_{0}_ruby'.format(i + 1)
    ]

  comp_fieldnames += [u'animedb_title', u'animedb_ruby', u'madb_id', u'madb_uri']

  comp_data = []

  for datum in anime_db:
    madb_id = datum.get(u'madb_id')

    comp_datum = {
      u'animedb_title': datum.get(u'title'),
      u'animedb_ruby': datum.get(u'ruby'),
      u'madb_id': madb_id,
      u'madb_uri': datum.get(u'madb_uri')
    }

    for i, madb in enumerate(madbs):
      if (not madb_id) or (not madb_id in madb):
        continue

      comp_datum[u'madb_{0}_title'.format(i + 1)] = madb[madb_id][u'タイトル']
      comp_datum[u'madb_{0}_ruby'.format(i + 1)] = madb[madb_id][u'よみがな']

    comp_data.append(comp_datum)

  unicode_csv_dict_writer(outputfile, comp_fieldnames, comp_data)

@cli.command('stats')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to show statistics.')
def stats(dbfile):
  data = load_db(dbfile)
  count = {}

  for medium in MEDIA:
    count[medium] = 0

  for datum in data:
    if not datum['medium'] in MEDIA:
      count[u'該当なし'] += 1
    else:
      count[datum['medium']] += 1

  total_count = len(data)
  table_rows = [['Medium', 'Count', 'Percent [%]']]

  for medium in MEDIA:
    table_rows.append([medium, count[medium], format_percentage(count[medium] * 1.0 / total_count)])

  table_rows.append(['Total', total_count, format_percentage(1)])

  table = SingleTable(table_rows)
  table.inner_footing_row_border = True

  print(table.table)

def listdb_default(data):
  for datum in data:
    print(u','.join([
      datum['id'],
      datum['medium'],
      format_date(datum['started_year'], datum['started_month'], datum['started_day']),
      format_date(datum['ended_year'], datum['ended_month'], datum['ended_day'])
    ]))

def listdb_google_ime(data):
  emitted_entry = {}

  for datum in data:
    if datum['ruby'] and datum['title']:
      ruby = re.sub(ur'[ ]', '', datum['ruby'])
      title = datum['title']
      entry = u'\t'.join([ruby, title, u'固有名詞'])

      if entry not in emitted_entry:
        print(entry)
        emitted_entry[entry] = True

def listdb_ms_ime(data):
  # Write BOM for Windows
  print(u'\ufeff'.encode('utf-16le'), end=u'')

  emitted_entry = {}

  for datum in data:
    if datum['ruby'] and datum['title']:
      ruby = jaconv.kata2hira(re.sub(ur'[ ]', '', datum['ruby'])).replace(u'ゔ', u'ヴ')
      title = datum['title']
      entry = u'\t'.join([ruby, title, u'固有名詞']).encode('utf-16le')

      if entry not in emitted_entry:
        print(entry, end=u'\r\n'.encode('utf-16le'))
        emitted_entry[entry] = True

def listdb_csv(data):
  writer = csv.DictWriter(sys.stdout, fieldnames=CSV_FIELDNAMES)

  writer.writeheader()

  for datum in data:
    for key, val in list(datum.items()):
      if isinstance(val, unicode):
        val = val.encode('utf-8')
      datum[key] = val

    writer.writerow({ key: datum.get(key, '') for key in CSV_FIELDNAMES })

@cli.command('list')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to list.')
@click.option('--sort', default=None, type=str, help='Comma-separated list of sorting keys.')
@click.option('--format', default=DEFAULT_LIST_FORMAT, type=str, help='A string to specify list format.')
def listdb(dbfile, sort, format):
  lister = globals().get('listdb_{0}'.format(format), None)

  if not lister:
    eprint('unknown list format: ', format)
    sys.exit(1)

  data = load_db(dbfile)

  if sort:
    data = sort_data(data, sort.split(','))

  lister(data)

def test_id_uniqueness(data):
  dic = {}

  for datum in data:
    if not datum['id'] in dic:
      dic[datum['id']] = [datum]
    else:
      dic[datum['id']] += datum

  dups = filter(lambda l: len(l) > 1, dic.values())

  if not dups:
    return True

  return '\n'.join(['duplicated IDs found:'] + map(lambda d: d[0]['id'], dups))

def test_name_uniqueness(data):
  def format_error(datum):
    return u'{0}: {1} {2}'.format(datum['id'], datum['title'], datum['suffix'])

  dic = {}

  for datum in data:
    name = u'{0} {1}'.format(datum['title'], datum['suffix'])

    if not name in dic:
      dic[name] = [datum]
    else:
      dic[name] += datum

  dups = filter(lambda l: len(l) > 1, dic.values())

  if not dups:
    return True

  return u'\n'.join(['duplicated names found:'] + map(lambda d: format_error(d[0]), dups))

def test_ruby(data):
  def anime_str(datum):
    return (datum['madb_id'] or '') + ':' + (datum['ruby'] or '') + ':' + (datum['title'] or '')

  def is_error(datum):
    return (re.search(ur'[^ァ-ヴー ]', datum['ruby']) or
            re.search(ur'[ ]{2,}', datum['ruby']) or
            re.search(ur'[ー]{2,}', datum['ruby']) or
            re.search(ur'^[ ]+', datum['ruby']) or
            re.search(ur'[ ]+$', datum['ruby']) or
            re.search(ur'^[ー]+', datum['ruby']))

  errors = []

  for datum in data:
    if (not datum['ruby']) or is_error(datum):
      errors.append(anime_str(datum))

  if not errors:
    return True

  return '\n'.join(['{0} invalid rubies found:'.format(len(errors))] + errors)

@cli.command()
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to test.')
def test(dbfile):
  data = load_db(dbfile)
  testers = filter(lambda (k, v): k.startswith('test_'), list(globals().items()))
  errors = []

  for name, tester in testers:
    result = tester(data)

    if result is not True:
      errors.append('{0} failed: '.format(name) + result)

  if errors:
    for error in errors:
      eprint(error)
    sys.exit(1)

@cli.command('fix_minnnanouta')
@click.option('--dbfile', default=DEFAULT_DBFILE, type=click.File('r'), help='Anime DB file to attach IDs to.')
@click.option('--outputfile', default=DEFAULT_CONVERTED_FILE, type=click.File('w'), help='Output file name.')
def fix_minnnanouta(dbfile, outputfile):
  data = load_db_roundtrip(dbfile)

  for datum in data:
    title = datum.get(u'title')
    ruby = datum.get(u'ruby')

    if not (title and ruby):
      continue

    if title.startswith(u'みんなのうた') and not ruby.startswith(u'ミンナノウタ'):
      datum[u'ruby'] = u'ミンナノウタ ' + ruby

  dump_db_roundtrip(data, outputfile)

if __name__ == '__main__':
  cli()
